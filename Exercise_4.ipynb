{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4\n",
    "\n",
    "**Match searches with bookings**\n",
    "\n",
    "- For every search in the searches file, find out whether the search ended up in a booking or not (using the info in the bookings file). For instance, search and booking origin and destination should match.\n",
    "- For the bookings file, origin and destination are the columns `dep_port` and `arr_port`, respectively.\n",
    "- Generate a CSV file with the search data, and an additional field, containing 1 if the search ended up in a booking, and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "#pd.options.display.max_rows = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_clean = ['Date', 'Seg1Departure', 'Seg1Arrival', 'Seg1Date', \n",
    "                 'Seg2Departure', 'Seg2Arrival', 'Seg2Date', \n",
    "                 'Seg3Departure', 'Seg3Arrival', 'Seg3Date', \n",
    "                 'Seg4Departure', 'Seg4Arrival', 'Seg4Date', \n",
    "                 'Seg5Departure', 'Seg5Arrival', 'Seg5Date', \n",
    "                 'Seg6Departure', 'Seg6Arrival', 'Seg6Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk: 1, size of chunk 358999\n",
      "Chunk: 2, size of chunk 358999\n",
      "Chunk: 3, size of chunk 359003\n",
      "Chunk: 4, size of chunk 359003\n",
      "Chunk: 5, size of chunk 359003\n",
      "Chunk: 6, size of chunk 359003\n",
      "Chunk: 7, size of chunk 359003\n",
      "Chunk: 8, size of chunk 359003\n",
      "Chunk: 9, size of chunk 359003\n",
      "Chunk: 10, size of chunk 359003\n",
      "Chunk: 11, size of chunk 359003\n",
      "Chunk: 12, size of chunk 359003\n",
      "Chunk: 13, size of chunk 359003\n",
      "Chunk: 14, size of chunk 359003\n",
      "Chunk: 15, size of chunk 359003\n",
      "Chunk: 16, size of chunk 359003\n",
      "Chunk: 17, size of chunk 359003\n",
      "Chunk: 18, size of chunk 359003\n",
      "Chunk: 19, size of chunk 359003\n",
      "Chunk: 20, size of chunk 359003\n",
      "Chunk: 21, size of chunk 359003\n",
      "Chunk: 22, size of chunk 359003\n",
      "Chunk: 23, size of chunk 359003\n",
      "Chunk: 24, size of chunk 359003\n",
      "Chunk: 25, size of chunk 359003\n",
      "Chunk: 26, size of chunk 359003\n",
      "Chunk: 27, size of chunk 359003\n",
      "Chunk: 28, size of chunk 359003\n",
      "Chunk: 29, size of chunk 359003\n",
      "Chunk: 30, size of chunk 359003\n",
      "Chunk: 31, size of chunk 359003\n",
      "Chunk: 32, size of chunk 359003\n",
      "Chunk: 33, size of chunk 359003\n",
      "Chunk: 34, size of chunk 359003\n",
      "Chunk: 35, size of chunk 359003\n",
      "Chunk: 36, size of chunk 359003\n",
      "Chunk: 37, size of chunk 359003\n",
      "Chunk: 38, size of chunk 359003\n",
      "Chunk: 39, size of chunk 359003\n",
      "Chunk: 40, size of chunk 359003\n",
      "Chunk: 41, size of chunk 359004\n"
     ]
    }
   ],
   "source": [
    "data_iterator = pd.read_csv(\"../challenge/searches.csv.bz2\", \n",
    "                            chunksize=5*10**5, \n",
    "                            sep='^', \n",
    "                            compression='bz2',\n",
    "                            #nrows=10**6,\n",
    "                            low_memory=True,\n",
    "                            dtype=str,\n",
    "                            #dtype=dtypes,\n",
    "                            #na_values=[np.nan],\n",
    "                            #memory_map=True\n",
    "                           )\n",
    "\n",
    "searches = pd.DataFrame()\n",
    "for i, data_chunk in enumerate(data_iterator):\n",
    "    #data_chunk = searches_data_preprocessing(data_chunk, columns_to_clean)\n",
    "    #data_chunk.reset_index(inplace=True)\n",
    "\n",
    "    searches = searches.append(data_chunk)\n",
    "    searches.drop_duplicates(inplace=True)\n",
    "    print(\"Chunk: %d, size of chunk %d\"%(i+1, searches.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>TxnCode</th>\n",
       "      <th>OfficeID</th>\n",
       "      <th>Country</th>\n",
       "      <th>Origin</th>\n",
       "      <th>Destination</th>\n",
       "      <th>RoundTrip</th>\n",
       "      <th>NbSegments</th>\n",
       "      <th>Seg1Departure</th>\n",
       "      <th>Seg1Arrival</th>\n",
       "      <th>Seg1Date</th>\n",
       "      <th>Seg1Carrier</th>\n",
       "      <th>Seg1BookingCode</th>\n",
       "      <th>Seg2Departure</th>\n",
       "      <th>Seg2Arrival</th>\n",
       "      <th>Seg2Date</th>\n",
       "      <th>Seg2Carrier</th>\n",
       "      <th>Seg2BookingCode</th>\n",
       "      <th>Seg3Departure</th>\n",
       "      <th>Seg3Arrival</th>\n",
       "      <th>Seg3Date</th>\n",
       "      <th>Seg3Carrier</th>\n",
       "      <th>Seg3BookingCode</th>\n",
       "      <th>Seg4Departure</th>\n",
       "      <th>Seg4Arrival</th>\n",
       "      <th>Seg4Date</th>\n",
       "      <th>Seg4Carrier</th>\n",
       "      <th>Seg4BookingCode</th>\n",
       "      <th>Seg5Departure</th>\n",
       "      <th>Seg5Arrival</th>\n",
       "      <th>Seg5Date</th>\n",
       "      <th>Seg5Carrier</th>\n",
       "      <th>Seg5BookingCode</th>\n",
       "      <th>Seg6Departure</th>\n",
       "      <th>Seg6Arrival</th>\n",
       "      <th>Seg6Date</th>\n",
       "      <th>Seg6Carrier</th>\n",
       "      <th>Seg6BookingCode</th>\n",
       "      <th>From</th>\n",
       "      <th>IsPublishedForNeg</th>\n",
       "      <th>IsFromInternet</th>\n",
       "      <th>IsFromVista</th>\n",
       "      <th>TerminalID</th>\n",
       "      <th>InternetOffice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1436000</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>20:25:57</td>\n",
       "      <td>MPT</td>\n",
       "      <td>624d8c3ac0b3a7ca03e3c167e0f48327</td>\n",
       "      <td>DE</td>\n",
       "      <td>TXL</td>\n",
       "      <td>AUH</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>TXL</td>\n",
       "      <td>AUH</td>\n",
       "      <td>2013-01-26</td>\n",
       "      <td>D2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AUH</td>\n",
       "      <td>TXL</td>\n",
       "      <td>2013-02-02</td>\n",
       "      <td>D2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1ASIWS</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>FRA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436001</th>\n",
       "      <td>2013-01-01,10:15:33,MPT,b0af35b31588dc4ab06d5c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436002</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>18:04:49</td>\n",
       "      <td>MPT</td>\n",
       "      <td>3561</td>\n",
       "      <td>US</td>\n",
       "      <td>ICT</td>\n",
       "      <td>SFO</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>ICT</td>\n",
       "      <td>SFO</td>\n",
       "      <td>2013-08-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SFO</td>\n",
       "      <td>ICT</td>\n",
       "      <td>2013-08-09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1ASIWS</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>NYC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436009</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>19:57:57</td>\n",
       "      <td>MPT</td>\n",
       "      <td>28d7a8c95e4db88589d3d35b66920e78</td>\n",
       "      <td>DE</td>\n",
       "      <td>FRA</td>\n",
       "      <td>BGW</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>FRA</td>\n",
       "      <td>BGW</td>\n",
       "      <td>2013-02-26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BGW</td>\n",
       "      <td>FRA</td>\n",
       "      <td>2013-04-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1ASI</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>BNJ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20390197</th>\n",
       "      <td>2013-10-13</td>\n",
       "      <td>18:57:54</td>\n",
       "      <td>MTP</td>\n",
       "      <td>e41c9d833aa74600552f2ed688b67d81</td>\n",
       "      <td>AT</td>\n",
       "      <td>VIE</td>\n",
       "      <td>HA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       Date      Time TxnCode  \\\n",
       "1436000                                          2013-01-01  20:25:57     MPT   \n",
       "1436001   2013-01-01,10:15:33,MPT,b0af35b31588dc4ab06d5c...       NaN     NaN   \n",
       "1436002                                          2013-01-01  18:04:49     MPT   \n",
       "1436009                                          2013-01-01  19:57:57     MPT   \n",
       "20390197                                         2013-10-13  18:57:54     MTP   \n",
       "\n",
       "                                  OfficeID Country Origin Destination  \\\n",
       "1436000   624d8c3ac0b3a7ca03e3c167e0f48327      DE    TXL         AUH   \n",
       "1436001                                NaN     NaN    NaN         NaN   \n",
       "1436002                               3561      US    ICT         SFO   \n",
       "1436009   28d7a8c95e4db88589d3d35b66920e78      DE    FRA         BGW   \n",
       "20390197  e41c9d833aa74600552f2ed688b67d81      AT    VIE          HA   \n",
       "\n",
       "         RoundTrip NbSegments Seg1Departure Seg1Arrival    Seg1Date  \\\n",
       "1436000          1          2           TXL         AUH  2013-01-26   \n",
       "1436001        NaN        NaN           NaN         NaN         NaN   \n",
       "1436002          1          2           ICT         SFO  2013-08-02   \n",
       "1436009          1          2           FRA         BGW  2013-02-26   \n",
       "20390197       NaN        NaN           NaN         NaN         NaN   \n",
       "\n",
       "         Seg1Carrier Seg1BookingCode Seg2Departure Seg2Arrival    Seg2Date  \\\n",
       "1436000           D2             NaN           AUH         TXL  2013-02-02   \n",
       "1436001          NaN             NaN           NaN         NaN         NaN   \n",
       "1436002          NaN             NaN           SFO         ICT  2013-08-09   \n",
       "1436009          NaN             NaN           BGW         FRA  2013-04-08   \n",
       "20390197         NaN             NaN           NaN         NaN         NaN   \n",
       "\n",
       "         Seg2Carrier Seg2BookingCode Seg3Departure Seg3Arrival Seg3Date  \\\n",
       "1436000           D2             NaN           NaN         NaN      NaN   \n",
       "1436001          NaN             NaN           NaN         NaN      NaN   \n",
       "1436002          NaN             NaN           NaN         NaN      NaN   \n",
       "1436009          NaN             NaN           NaN         NaN      NaN   \n",
       "20390197         NaN             NaN           NaN         NaN      NaN   \n",
       "\n",
       "         Seg3Carrier Seg3BookingCode Seg4Departure Seg4Arrival Seg4Date  \\\n",
       "1436000          NaN             NaN           NaN         NaN      NaN   \n",
       "1436001          NaN             NaN           NaN         NaN      NaN   \n",
       "1436002          NaN             NaN           NaN         NaN      NaN   \n",
       "1436009          NaN             NaN           NaN         NaN      NaN   \n",
       "20390197         NaN             NaN           NaN         NaN      NaN   \n",
       "\n",
       "         Seg4Carrier Seg4BookingCode Seg5Departure Seg5Arrival Seg5Date  \\\n",
       "1436000          NaN             NaN           NaN         NaN      NaN   \n",
       "1436001          NaN             NaN           NaN         NaN      NaN   \n",
       "1436002          NaN             NaN           NaN         NaN      NaN   \n",
       "1436009          NaN             NaN           NaN         NaN      NaN   \n",
       "20390197         NaN             NaN           NaN         NaN      NaN   \n",
       "\n",
       "         Seg5Carrier Seg5BookingCode Seg6Departure Seg6Arrival Seg6Date  \\\n",
       "1436000          NaN             NaN           NaN         NaN   1ASIWS   \n",
       "1436001          NaN             NaN           NaN         NaN      NaN   \n",
       "1436002          NaN             NaN           NaN         NaN      NaN   \n",
       "1436009          NaN             NaN           NaN         NaN      NaN   \n",
       "20390197         NaN             NaN           NaN         NaN      NaN   \n",
       "\n",
       "         Seg6Carrier Seg6BookingCode    From  \\\n",
       "1436000            0               0       0   \n",
       "1436001          NaN             NaN     NaN   \n",
       "1436002          NaN             NaN  1ASIWS   \n",
       "1436009         1ASI               0       0   \n",
       "20390197         NaN             NaN     NaN   \n",
       "\n",
       "                         IsPublishedForNeg                    IsFromInternet  \\\n",
       "1436000   d41d8cd98f00b204e9800998ecf8427e                               FRA   \n",
       "1436001                                NaN                               NaN   \n",
       "1436002                                  0                                 0   \n",
       "1436009                                  0  d41d8cd98f00b204e9800998ecf8427e   \n",
       "20390197                               NaN                               NaN   \n",
       "\n",
       "         IsFromVista                        TerminalID InternetOffice  \n",
       "1436000          NaN                               NaN            NaN  \n",
       "1436001          NaN                               NaN            NaN  \n",
       "1436002            0  d41d8cd98f00b204e9800998ecf8427e            NYC  \n",
       "1436009          BNJ                               NaN            NaN  \n",
       "20390197         NaN                               NaN            NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searches.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "searches.to_csv('../challenge/searches_no_dups.csv', sep='^', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searches_data_preprocessing(df, columns_to_clean): \n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df.dropna(inplace=True, subset=['Date', 'Seg1Departure'])\n",
    "    \n",
    "    for column in columns_to_clean:\n",
    "        df[column] = df[column].str.strip()\n",
    "    \n",
    "        if 'Date' not in column:\n",
    "            df[column] = df[column].str.upper()\n",
    "            # Para saber si hay lengths superiores a 3 sin que nos de error por los missing values    \n",
    "            cell_lengths = df[column].map(lambda x: len(x) if not isinstance(x, float) else x)\n",
    "            cell_lengths_notna = cell_lengths[cell_lengths.notna()]\n",
    "            cell_lengths_sum = (cell_lengths_notna != 3.0).sum()\n",
    "            if cell_lengths_sum != 0:\n",
    "                print(f'Alert! There are `{column}` values with length different than 3. Dropping them..')\n",
    "\n",
    "                mask = cell_lengths_notna != 3.0\n",
    "                wrong_data = cell_lengths_notna[mask]\n",
    "                print(wrong_data)\n",
    "                df.drop(index=wrong_data.index, inplace=True)\n",
    "    return df      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk: 1, size of chunk 100000\n",
      "Chunk: 2, size of chunk 200000\n",
      "Chunk: 3, size of chunk 300000\n",
      "Chunk: 4, size of chunk 359002\n"
     ]
    }
   ],
   "source": [
    "columns_to_clean = ['Date', 'Seg1Departure', 'Seg1Arrival', 'Seg1Date', \n",
    "                 'Seg2Departure', 'Seg2Arrival', 'Seg2Date', \n",
    "                 'Seg3Departure', 'Seg3Arrival', 'Seg3Date', \n",
    "                 'Seg4Departure', 'Seg4Arrival', 'Seg4Date', \n",
    "                 'Seg5Departure', 'Seg5Arrival', 'Seg5Date', \n",
    "                 'Seg6Departure', 'Seg6Arrival', 'Seg6Date',\n",
    "                ]\n",
    "\n",
    "data_iterator = pd.read_csv(\"../challenge/searches_no_dups.csv\", \n",
    "                            chunksize=10**5, \n",
    "                            sep='^', \n",
    "                            #compression='bz2',\n",
    "                            #nrows=10**6,\n",
    "                            low_memory=False,\n",
    "                            #dtype=str,\n",
    "                            #dtype=dtypes,\n",
    "                            #na_values=[np.nan],\n",
    "                            #memory_map=True\n",
    "                           )\n",
    "\n",
    "searches = pd.DataFrame()\n",
    "for i, data_chunk in enumerate(data_iterator):\n",
    "    data_chunk = searches_data_preprocessing(data_chunk, columns_to_clean)\n",
    "    #data_chunk.reset_index(inplace=True)\n",
    "    searches = searches.append(data_chunk)\n",
    "    #searches.drop_duplicates(inplace=True)\n",
    "    print(\"Chunk: %d, size of chunk %d\"%(i+1, searches.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(359002, 45)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>TxnCode</th>\n",
       "      <th>OfficeID</th>\n",
       "      <th>Country</th>\n",
       "      <th>Origin</th>\n",
       "      <th>Destination</th>\n",
       "      <th>RoundTrip</th>\n",
       "      <th>NbSegments</th>\n",
       "      <th>Seg1Departure</th>\n",
       "      <th>Seg1Arrival</th>\n",
       "      <th>Seg1Date</th>\n",
       "      <th>Seg1Carrier</th>\n",
       "      <th>Seg1BookingCode</th>\n",
       "      <th>Seg2Departure</th>\n",
       "      <th>Seg2Arrival</th>\n",
       "      <th>Seg2Date</th>\n",
       "      <th>Seg2Carrier</th>\n",
       "      <th>Seg2BookingCode</th>\n",
       "      <th>Seg3Departure</th>\n",
       "      <th>Seg3Arrival</th>\n",
       "      <th>Seg3Date</th>\n",
       "      <th>Seg3Carrier</th>\n",
       "      <th>Seg3BookingCode</th>\n",
       "      <th>Seg4Departure</th>\n",
       "      <th>Seg4Arrival</th>\n",
       "      <th>Seg4Date</th>\n",
       "      <th>Seg4Carrier</th>\n",
       "      <th>Seg4BookingCode</th>\n",
       "      <th>Seg5Departure</th>\n",
       "      <th>Seg5Arrival</th>\n",
       "      <th>Seg5Date</th>\n",
       "      <th>Seg5Carrier</th>\n",
       "      <th>Seg5BookingCode</th>\n",
       "      <th>Seg6Departure</th>\n",
       "      <th>Seg6Arrival</th>\n",
       "      <th>Seg6Date</th>\n",
       "      <th>Seg6Carrier</th>\n",
       "      <th>Seg6BookingCode</th>\n",
       "      <th>From</th>\n",
       "      <th>IsPublishedForNeg</th>\n",
       "      <th>IsFromInternet</th>\n",
       "      <th>IsFromVista</th>\n",
       "      <th>TerminalID</th>\n",
       "      <th>InternetOffice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>358998</th>\n",
       "      <td>2013-12-25</td>\n",
       "      <td>20:54:39</td>\n",
       "      <td>FXA</td>\n",
       "      <td>7f8c2d9dfe430c2c6b19298dd7e0ff96</td>\n",
       "      <td>US</td>\n",
       "      <td>LAX</td>\n",
       "      <td>TLV</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>LAX</td>\n",
       "      <td>AMS</td>\n",
       "      <td>2014-02-02</td>\n",
       "      <td>UV</td>\n",
       "      <td>J</td>\n",
       "      <td>AMS</td>\n",
       "      <td>TLV</td>\n",
       "      <td>2014-02-03</td>\n",
       "      <td>UV</td>\n",
       "      <td>J</td>\n",
       "      <td>TLV</td>\n",
       "      <td>AMS</td>\n",
       "      <td>2014-02-17</td>\n",
       "      <td>UV</td>\n",
       "      <td>J</td>\n",
       "      <td>AMS</td>\n",
       "      <td>ATL</td>\n",
       "      <td>2014-02-17</td>\n",
       "      <td>UV</td>\n",
       "      <td>J</td>\n",
       "      <td>ATL</td>\n",
       "      <td>LAX</td>\n",
       "      <td>2014-02-17</td>\n",
       "      <td>UV</td>\n",
       "      <td>J</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1ASI</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358999</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>20:25:57</td>\n",
       "      <td>MPT</td>\n",
       "      <td>624d8c3ac0b3a7ca03e3c167e0f48327</td>\n",
       "      <td>DE</td>\n",
       "      <td>TXL</td>\n",
       "      <td>AUH</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>TXL</td>\n",
       "      <td>AUH</td>\n",
       "      <td>2013-01-26</td>\n",
       "      <td>D2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AUH</td>\n",
       "      <td>TXL</td>\n",
       "      <td>2013-02-02</td>\n",
       "      <td>D2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1ASIWS</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>FRA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359001</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>18:04:49</td>\n",
       "      <td>MPT</td>\n",
       "      <td>3561</td>\n",
       "      <td>US</td>\n",
       "      <td>ICT</td>\n",
       "      <td>SFO</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>ICT</td>\n",
       "      <td>SFO</td>\n",
       "      <td>2013-08-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SFO</td>\n",
       "      <td>ICT</td>\n",
       "      <td>2013-08-09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1ASIWS</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>NYC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359002</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>19:57:57</td>\n",
       "      <td>MPT</td>\n",
       "      <td>28d7a8c95e4db88589d3d35b66920e78</td>\n",
       "      <td>DE</td>\n",
       "      <td>FRA</td>\n",
       "      <td>BGW</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>FRA</td>\n",
       "      <td>BGW</td>\n",
       "      <td>2013-02-26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BGW</td>\n",
       "      <td>FRA</td>\n",
       "      <td>2013-04-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1ASI</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>BNJ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359003</th>\n",
       "      <td>2013-10-13</td>\n",
       "      <td>18:57:54</td>\n",
       "      <td>MTP</td>\n",
       "      <td>e41c9d833aa74600552f2ed688b67d81</td>\n",
       "      <td>AT</td>\n",
       "      <td>VIE</td>\n",
       "      <td>HA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Date      Time TxnCode                          OfficeID  \\\n",
       "358998  2013-12-25  20:54:39     FXA  7f8c2d9dfe430c2c6b19298dd7e0ff96   \n",
       "358999  2013-01-01  20:25:57     MPT  624d8c3ac0b3a7ca03e3c167e0f48327   \n",
       "359001  2013-01-01  18:04:49     MPT                              3561   \n",
       "359002  2013-01-01  19:57:57     MPT  28d7a8c95e4db88589d3d35b66920e78   \n",
       "359003  2013-10-13  18:57:54     MTP  e41c9d833aa74600552f2ed688b67d81   \n",
       "\n",
       "       Country Origin Destination  RoundTrip  NbSegments Seg1Departure  \\\n",
       "358998      US    LAX         TLV        1.0         5.0           LAX   \n",
       "358999      DE    TXL         AUH        1.0         2.0           TXL   \n",
       "359001      US    ICT         SFO        1.0         2.0           ICT   \n",
       "359002      DE    FRA         BGW        1.0         2.0           FRA   \n",
       "359003      AT    VIE          HA        NaN         NaN           NaN   \n",
       "\n",
       "       Seg1Arrival    Seg1Date Seg1Carrier Seg1BookingCode Seg2Departure  \\\n",
       "358998         AMS  2014-02-02          UV               J           AMS   \n",
       "358999         AUH  2013-01-26          D2             NaN           AUH   \n",
       "359001         SFO  2013-08-02         NaN             NaN           SFO   \n",
       "359002         BGW  2013-02-26         NaN             NaN           BGW   \n",
       "359003         NaN         NaN         NaN             NaN           NaN   \n",
       "\n",
       "       Seg2Arrival    Seg2Date Seg2Carrier Seg2BookingCode Seg3Departure  \\\n",
       "358998         TLV  2014-02-03          UV               J           TLV   \n",
       "358999         TXL  2013-02-02          D2             NaN           NaN   \n",
       "359001         ICT  2013-08-09         NaN             NaN           NaN   \n",
       "359002         FRA  2013-04-08         NaN             NaN           NaN   \n",
       "359003         NaN         NaN         NaN             NaN           NaN   \n",
       "\n",
       "       Seg3Arrival    Seg3Date Seg3Carrier Seg3BookingCode Seg4Departure  \\\n",
       "358998         AMS  2014-02-17          UV               J           AMS   \n",
       "358999         NaN         NaN         NaN             NaN           NaN   \n",
       "359001         NaN         NaN         NaN             NaN           NaN   \n",
       "359002         NaN         NaN         NaN             NaN           NaN   \n",
       "359003         NaN         NaN         NaN             NaN           NaN   \n",
       "\n",
       "       Seg4Arrival    Seg4Date Seg4Carrier Seg4BookingCode Seg5Departure  \\\n",
       "358998         ATL  2014-02-17          UV               J           ATL   \n",
       "358999         NaN         NaN         NaN             NaN           NaN   \n",
       "359001         NaN         NaN         NaN             NaN           NaN   \n",
       "359002         NaN         NaN         NaN             NaN           NaN   \n",
       "359003         NaN         NaN         NaN             NaN           NaN   \n",
       "\n",
       "       Seg5Arrival    Seg5Date Seg5Carrier Seg5BookingCode Seg6Departure  \\\n",
       "358998         LAX  2014-02-17          UV               J           NaN   \n",
       "358999         NaN         NaN         NaN             NaN           NaN   \n",
       "359001         NaN         NaN         NaN             NaN           NaN   \n",
       "359002         NaN         NaN         NaN             NaN           NaN   \n",
       "359003         NaN         NaN         NaN             NaN           NaN   \n",
       "\n",
       "       Seg6Arrival Seg6Date Seg6Carrier Seg6BookingCode    From  \\\n",
       "358998         NaN      NaN         NaN             NaN    1ASI   \n",
       "358999         NaN   1ASIWS           0               0       0   \n",
       "359001         NaN      NaN         NaN             NaN  1ASIWS   \n",
       "359002         NaN      NaN        1ASI               0       0   \n",
       "359003         NaN      NaN         NaN             NaN     NaN   \n",
       "\n",
       "                       IsPublishedForNeg                    IsFromInternet  \\\n",
       "358998                                 0                                 0   \n",
       "358999  d41d8cd98f00b204e9800998ecf8427e                               FRA   \n",
       "359001                                 0                                 0   \n",
       "359002                                 0  d41d8cd98f00b204e9800998ecf8427e   \n",
       "359003                               NaN                               NaN   \n",
       "\n",
       "       IsFromVista                        TerminalID InternetOffice  \n",
       "358998           0  d41d8cd98f00b204e9800998ecf8427e              0  \n",
       "358999         NaN                               NaN            NaN  \n",
       "359001           0  d41d8cd98f00b204e9800998ecf8427e            NYC  \n",
       "359002         BNJ                               NaN            NaN  \n",
       "359003         NaN                               NaN            NaN  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searches.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk: 1, size of chunk 354095\n",
      "Chunk: 2, size of chunk 708879\n",
      "Chunk: 3, size of chunk 708879\n",
      "Chunk: 4, size of chunk 708879\n",
      "Chunk: 5, size of chunk 708879\n",
      "Chunk: 6, size of chunk 708879\n",
      "Chunk: 7, size of chunk 708879\n",
      "Chunk: 8, size of chunk 708879\n",
      "Chunk: 9, size of chunk 708879\n",
      "Chunk: 10, size of chunk 708879\n",
      "Chunk: 11, size of chunk 708882\n",
      "Chunk: 12, size of chunk 708882\n",
      "Chunk: 13, size of chunk 708882\n",
      "Chunk: 14, size of chunk 708882\n",
      "Chunk: 15, size of chunk 708882\n",
      "Chunk: 16, size of chunk 708882\n",
      "Chunk: 17, size of chunk 708882\n",
      "Chunk: 18, size of chunk 708882\n",
      "Chunk: 19, size of chunk 708882\n",
      "Chunk: 20, size of chunk 708882\n",
      "Chunk: 21, size of chunk 708882\n"
     ]
    }
   ],
   "source": [
    "data_iterator = pd.read_csv(\"../challenge/bookings.csv.bz2\", \n",
    "                            chunksize = 5*10**5, \n",
    "                            sep = '^', \n",
    "                            compression = 'bz2',\n",
    "                            #nrows = 10**6,\n",
    "                            usecols = ['dep_port', \n",
    "                                       'arr_port', \n",
    "                                       'brd_time           ',\n",
    "                                       'cre_date           ',\n",
    "                                      ],\n",
    "                            low_memory=True,\n",
    "                            dtype=str,\n",
    "                           )\n",
    "\n",
    "bookings = pd.DataFrame()\n",
    "for i, data_chunk in enumerate(data_iterator):\n",
    "    #data_chunk = bookings_data_preprocessing(data_chunk)\n",
    "    #data_chunk.reset_index(inplace=True)\n",
    "    #data_chunk['Booking'] = 1\n",
    "    bookings = bookings.append(data_chunk)\n",
    "    bookings.drop_duplicates(inplace=True)\n",
    "    print(\"Chunk: %d, size of chunk %d\"%(i+1, bookings.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookings.to_csv('../challenge/bookings_no_dups.csv', sep='^', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bookings_data_preprocessing(df): \n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    #df.drop_duplicates(inplace=True)\n",
    "    df.dropna(inplace=True, subset=['cre_date           '])\n",
    "    \n",
    "    for column in df.columns:\n",
    "        df[column] = df[column].str.strip()\n",
    "    \n",
    "    \n",
    "        if 'port' in column:\n",
    "            df[column] = df[column].str.upper()\n",
    "            \n",
    "            # Para saber si hay lengths superiores a 3 sin que nos de error por los missing values\n",
    "            cell_lengths = df[column].map(lambda x: len(x) if not isinstance(x, float) else x)\n",
    "            cell_lengths_notna = cell_lengths[cell_lengths.notna()]\n",
    "            cell_lengths_sum = (cell_lengths_notna != 3.0).sum()\n",
    "            if cell_lengths_sum != 0:\n",
    "                print(f'Alert! There are `{column}` values with length different than 3. Dropping them..')\n",
    "\n",
    "                mask = cell_lengths_notna != 3.0\n",
    "                wrong_data = cell_lengths_notna[mask]\n",
    "                print(wrong_data)\n",
    "                df.drop(index=wrong_data.index, inplace=True)\n",
    "        else:\n",
    "            df[column] = df[column].str.split().str[0]\n",
    "   #df.dropna(inplace=True, subset=['cre_date'])\n",
    "        \n",
    "    return df      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk: 1, size of chunk 100000\n",
      "Chunk: 2, size of chunk 200000\n",
      "Chunk: 3, size of chunk 300000\n",
      "Chunk: 4, size of chunk 400000\n",
      "Chunk: 5, size of chunk 500000\n",
      "Chunk: 6, size of chunk 600000\n",
      "Chunk: 7, size of chunk 700000\n",
      "Alert! There are `dep_port` values with length different than 3. Dropping them..\n",
      "708880    2\n",
      "Name: dep_port, dtype: int64\n",
      "Chunk: 8, size of chunk 708881\n"
     ]
    }
   ],
   "source": [
    "data_iterator = pd.read_csv(\"../challenge/bookings_no_dups.csv\", \n",
    "                            chunksize = 10**5, \n",
    "                            sep = '^', \n",
    "                            #compression = 'bz2',\n",
    "                            #nrows = 10**6,\n",
    "                            usecols = ['dep_port', \n",
    "                                       'arr_port', \n",
    "                                       'brd_time           ',\n",
    "                                       'cre_date           ',\n",
    "                                      ],\n",
    "                            #low_memory=False,\n",
    "                            #parse_dates=['cre_date           '],\n",
    "                           )\n",
    "\n",
    "bookings = pd.DataFrame()\n",
    "for i, data_chunk in enumerate(data_iterator):\n",
    "    data_chunk = bookings_data_preprocessing(data_chunk)\n",
    "    #data_chunk.reset_index(inplace=True)\n",
    "    data_chunk['Booking'] = 1\n",
    "    bookings = bookings.append(data_chunk)\n",
    "    print(\"Chunk: %d, size of chunk %d\"%(i+1, bookings.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(708881, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bookings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cre_date</th>\n",
       "      <th>dep_port</th>\n",
       "      <th>arr_port</th>\n",
       "      <th>brd_time</th>\n",
       "      <th>Booking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>708876</th>\n",
       "      <td>2013-05-28</td>\n",
       "      <td>AUS</td>\n",
       "      <td>RDU</td>\n",
       "      <td>2013-07-12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708877</th>\n",
       "      <td>2013-05-28</td>\n",
       "      <td>TLS</td>\n",
       "      <td>ORY</td>\n",
       "      <td>2013-06-04</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708878</th>\n",
       "      <td>2013-05-28</td>\n",
       "      <td>TLS</td>\n",
       "      <td>ORY</td>\n",
       "      <td>2013-06-04</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708879</th>\n",
       "      <td>2013-03-26</td>\n",
       "      <td>AKL</td>\n",
       "      <td>SVO</td>\n",
       "      <td>2013-04-24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708881</th>\n",
       "      <td>2013-03-25,00:00:00</td>\n",
       "      <td>TYO</td>\n",
       "      <td>SIN</td>\n",
       "      <td>2013-04-16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        cre_date            dep_port arr_port brd_time             Booking\n",
       "708876           2013-05-28      AUS      RDU          2013-07-12        1\n",
       "708877           2013-05-28      TLS      ORY          2013-06-04        1\n",
       "708878           2013-05-28      TLS      ORY          2013-06-04        1\n",
       "708879           2013-03-26      AKL      SVO          2013-04-24        1\n",
       "708881  2013-03-25,00:00:00      TYO      SIN          2013-04-16        1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bookings.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobar de nuevo los duplicados y eliminarlos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49882"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bookings.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searches.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bookings.drop_duplicates(inplace=True)\n",
    "bookings.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cre_date</th>\n",
       "      <th>dep_port</th>\n",
       "      <th>arr_port</th>\n",
       "      <th>brd_time</th>\n",
       "      <th>Booking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>708874</th>\n",
       "      <td>2013-05-14</td>\n",
       "      <td>CMB</td>\n",
       "      <td>MAA</td>\n",
       "      <td>2013-05-24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708875</th>\n",
       "      <td>2013-05-14</td>\n",
       "      <td>CMB</td>\n",
       "      <td>MAA</td>\n",
       "      <td>2013-05-19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708876</th>\n",
       "      <td>2013-05-28</td>\n",
       "      <td>AUS</td>\n",
       "      <td>RDU</td>\n",
       "      <td>2013-07-12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708877</th>\n",
       "      <td>2013-05-28</td>\n",
       "      <td>TLS</td>\n",
       "      <td>ORY</td>\n",
       "      <td>2013-06-04</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708881</th>\n",
       "      <td>2013-03-25,00:00:00</td>\n",
       "      <td>TYO</td>\n",
       "      <td>SIN</td>\n",
       "      <td>2013-04-16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        cre_date            dep_port arr_port brd_time             Booking\n",
       "708874           2013-05-14      CMB      MAA          2013-05-24        1\n",
       "708875           2013-05-14      CMB      MAA          2013-05-19        1\n",
       "708876           2013-05-28      AUS      RDU          2013-07-12        1\n",
       "708877           2013-05-28      TLS      ORY          2013-06-04        1\n",
       "708881  2013-03-25,00:00:00      TYO      SIN          2013-04-16        1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bookings.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(658999, 5)\n",
      "(359003, 45)\n"
     ]
    }
   ],
   "source": [
    "print(bookings.shape)\n",
    "print(searches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns_bookings = [['Date', 'Seg1Departure', 'Seg1Arrival', 'Seg1Date', 'booking'], \n",
    "                        ['Date', 'Seg2Departure', 'Seg2Arrival', 'Seg2Date', 'booking'],\n",
    "                        ['Date', 'Seg3Departure', 'Seg3Arrival', 'Seg3Date', 'booking'],\n",
    "                        ['Date', 'Seg4Departure', 'Seg4Arrival', 'Seg4Date', 'booking'],\n",
    "                        ['Date', 'Seg5Departure', 'Seg5Arrival', 'Seg5Date', 'booking'],\n",
    "                        ['Date', 'Seg6Departure', 'Seg6Arrival', 'Seg6Date', 'booking'],\n",
    "                       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(359003, 46)\n",
      "(359003, 47)\n",
      "(359003, 48)\n",
      "(359003, 49)\n",
      "(359003, 50)\n",
      "(359003, 51)\n"
     ]
    }
   ],
   "source": [
    "for i, lista in enumerate(new_columns_bookings):\n",
    "    bookings.columns = lista\n",
    "\n",
    "    searches = searches.merge(bookings, \n",
    "                              how = 'left', \n",
    "                              on = lista[:4], \n",
    "                              suffixes=(i, i+1),\n",
    "                             )\n",
    "    print(searches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 359003 entries, 0 to 359002\n",
      "Data columns (total 6 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   booking1  451 non-null    float64\n",
      " 1   booking2  181 non-null    float64\n",
      " 2   booking3  31 non-null     float64\n",
      " 3   booking4  25 non-null     float64\n",
      " 4   booking5  3 non-null      float64\n",
      " 5   booking6  1 non-null      float64\n",
      "dtypes: float64(6)\n",
      "memory usage: 19.2 MB\n"
     ]
    }
   ],
   "source": [
    "searches.iloc[:, -6:].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "searches.iloc[:, -6:] = searches.iloc[:, -6:].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 359003 entries, 0 to 359002\n",
      "Data columns (total 6 columns):\n",
      " #   Column    Non-Null Count   Dtype  \n",
      "---  ------    --------------   -----  \n",
      " 0   booking1  359003 non-null  float64\n",
      " 1   booking2  359003 non-null  float64\n",
      " 2   booking3  359003 non-null  float64\n",
      " 3   booking4  359003 non-null  float64\n",
      " 4   booking5  359003 non-null  float64\n",
      " 5   booking6  359003 non-null  float64\n",
      "dtypes: float64(6)\n",
      "memory usage: 19.2 MB\n"
     ]
    }
   ],
   "source": [
    "searches.iloc[:, -6:].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "booking1    451.0\n",
       "booking2    181.0\n",
       "booking3     31.0\n",
       "booking4     25.0\n",
       "booking5      3.0\n",
       "booking6      1.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searches.iloc[:, -6:].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos la nueva columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "searches['Booking'] = 0\n",
    "for column in searches.columns[-7:-1]:\n",
    "    searches['Booking'] += searches[column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "692.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searches['Booking'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "692.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searches.iloc[:, -7:-1].sum().sum() #comprobacion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos las columnas sobrantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['booking1', 'booking2', 'booking3', 'booking4', 'booking5', 'booking6'], dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searches.columns[-7:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(359003, 52)\n",
      "(359003, 46)\n"
     ]
    }
   ],
   "source": [
    "print(searches.shape)\n",
    "searches.drop(columns=searches.columns[-7:-1], inplace=True)\n",
    "print(searches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "692.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searches['Booking'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cre_date               0\n",
       "dep_port               0\n",
       "arr_port               0\n",
       "brd_time               0\n",
       "Booking                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bookings.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date                      0\n",
       "Time                      0\n",
       "TxnCode                   0\n",
       "OfficeID                  0\n",
       "Country                  51\n",
       "Origin                    0\n",
       "Destination               0\n",
       "RoundTrip                 0\n",
       "NbSegments                0\n",
       "Seg1Departure             0\n",
       "Seg1Arrival               0\n",
       "Seg1Date               1282\n",
       "Seg1Carrier          284543\n",
       "Seg1BookingCode      411897\n",
       "Seg2Departure        132239\n",
       "Seg2Arrival          132239\n",
       "Seg2Date             133765\n",
       "Seg2Carrier          337279\n",
       "Seg2BookingCode      418449\n",
       "Seg3Departure        434559\n",
       "Seg3Arrival          434559\n",
       "Seg3Date             434691\n",
       "Seg3Carrier          435786\n",
       "Seg3BookingCode      437078\n",
       "Seg4Departure        439480\n",
       "Seg4Arrival          439480\n",
       "Seg4Date             439599\n",
       "Seg4Carrier          439648\n",
       "Seg4BookingCode      439945\n",
       "Seg5Departure        455221\n",
       "Seg5Arrival          455221\n",
       "Seg5Date             455249\n",
       "Seg5Carrier          455244\n",
       "Seg5BookingCode      455309\n",
       "Seg6Departure        456732\n",
       "Seg6Arrival          456732\n",
       "Seg6Date             456754\n",
       "Seg6Carrier          456735\n",
       "Seg6BookingCode      456776\n",
       "From                   8204\n",
       "IsPublishedForNeg         0\n",
       "IsFromInternet            0\n",
       "IsFromVista               0\n",
       "TerminalID                0\n",
       "InternetOffice            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searches.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Column|NaN|\n",
    "|:---:|:---:|\n",
    "|Date|0|\n",
    "|Seg1Departure|0|\n",
    "|Seg1Arrival|0|\n",
    "|Seg1Date|1282|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Column|NaN|\n",
    "|:---:|:---:|\n",
    "|Date|0|\n",
    "|Seg2Departure|132239|\n",
    "|Seg2Arrival|132239|\n",
    "|Seg2Date|133765|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pdemos observar el mismo patron en todos los segmentos, `SegnDeparture` y `SegnArribal` tienen la misma cantidad de missing values, mientras que `SegnDate` tiene unos pocos mas. \n",
    "\n",
    "Ello significa que si un cierto dia se reservan vuelos de una ciudad a otra en dias distintos, estos tendran los mismos datos en las columnas relevantes para hacer el join.\n",
    "\n",
    "Esto lo vamos a dejar para el finde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
